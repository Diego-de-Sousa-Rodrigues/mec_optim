{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Crash course 4: Generalized linear models</center>\n",
    "\n",
    "### <center>Alfred Galichon (NYU & Sciences Po)</center>\n",
    "## <center>'math+econ+code' masterclass on optimal transport and economic applications</center>\n",
    "#### <center>With python code examples</center>\n",
    "Â© 2018-2022 by Alfred Galichon. Past and present support from NSF grant DMS-1716489, ERC grant CoG-866274 are acknowledged, as well as inputs from contributors listed [here](http://www.math-econ-code.org/theteam).\n",
    "\n",
    "**If you reuse material from this masterclass, please cite as:**<br>\n",
    "Alfred Galichon, 'math+econ+code' masterclass on optimal transport and economic applications, January 2022. https://github.com/math-econ-code/mec_optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* McCullagh and Nelder (1989). Generalized Linear Models (2nd ed.). Chapman and Hall/CRC.\n",
    "* Friedman, Tibshirani, and Hastie (2001). The Elements of Statistical Learning. Springer.\n",
    "* The Scikit-learn library www.scikit-learn.org.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized linear models\n",
    "## Setting\n",
    "\n",
    "* In many setting, an economic model will allow to make predictions on the\n",
    "conditional mean of a dependent random variable $y$ given explanatory random\n",
    "vector $x$.\n",
    "\n",
    "* In the case of linear regression, we have\n",
    "$$E\\left[  y|x\\right]  =x^{\\top}\\beta$$\n",
    "however, we shall encounter situations where it will be useful to be more general.\n",
    "\n",
    "* This leads us to *generalized linear models* (GLM), which are specified as\n",
    "\n",
    "$$E\\left[  y|x\\right]  =g^{-1}\\left(  x^{\\top}\\beta\\right)$$\n",
    "\n",
    "where $g:\\mathbb{R}\\rightarrow\\mathbb{R}$ is an increasing and continuous\n",
    "function called *link function*.\n",
    "\n",
    "* Often we shall specify in addition $Var\\left(  y|x\\right)  =V\\left(\n",
    "g^{-1}\\left(  x^{\\top}\\beta\\right)  \\right)  $.\n",
    "\n",
    "We shall use `linear_model`from the scikit-learn library `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Example 1: ordinary least squares (OLS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* In least squares (OLS), we have $$y=x^{\\top}\\beta+\\epsilon$$\n",
    "with $E\\left[  \\epsilon|x\\right]  =0$, in which case $g\\left(  z\\right)  =z$.\n",
    "\n",
    "* Additionally, assuming $E\\left[  \\epsilon^{2}|x\\right]  =\\sigma^{2}$, we\n",
    "have \n",
    "$$Var\\left(  y|x\\right)  =\\sigma^{2}.$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS regression in scikit-learn\n",
    "\n",
    "The following example is taken from the `scikit-learn` documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example taken from https://scikit-learn.org/0.15/modules/linear_model.html\n",
    "clf = linear_model.LinearRegression()\n",
    "clf.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
    "clf.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Poisson regression\n",
    "\n",
    "\n",
    "\n",
    "* Recall a Poisson distribution with parameter $\\theta\\in(0,+\\infty)$ has\n",
    "probability mass \n",
    "\n",
    "$$\\pi_{z|\\theta}=\\frac{e^{-\\theta}\\theta^{z}}{z!}$$\n",
    "\n",
    "over $z\\in\\left\\{  0,1,2,...\\right\\}  $. It has expectation and variance\n",
    "$\\theta$.\n",
    "\n",
    "* Assume that conditional on $x$, $y$ has a Poisson distribution of\n",
    "parameter $\\theta=\\exp\\left(  x^{\\top}\\beta\\right)  $. Then\n",
    "$$ E\\left[  y|x\\right]  =\\exp\\left(  x^{\\top}\\beta\\right)$$\n",
    "so in this case $g=\\ln$.\n",
    "\n",
    "* Note that we get $$var\\left(  y|x\\right)  =\\exp\\left(  x^{\\top}\\beta\\right)$$\n",
    "which may be overrestrictive (more on this later).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson regression\n",
    "\n",
    "\n",
    "\n",
    "* Sample log-likelihood\n",
    "$$\n",
    "\\sum_{i}-\\exp\\left(  x_{i}^{\\top}\\beta\\right)  +x_{i}^{\\top}\\beta y_{i}%\n",
    "-\\ln\\left(  y_{i}!\\right)\n",
    "$$\n",
    "and therefore, max likelihood yields the Poisson regression\n",
    "$$\n",
    "\\max_{\\beta}\\left\\{  \\sum_{i}-\\exp\\left(  x_{i}^{\\top}\\beta\\right)\n",
    "+x_{i}^{\\top}\\beta y_{i}\\right\\}\n",
    "$$\n",
    "\n",
    "\n",
    "* First order conditions give\n",
    "$$\n",
    "\\sum_{i}\\left(  y_{i}-\\exp\\left(  x_{i}^{\\top}\\beta\\right)  \\right)  x_{i}=0.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson regression in scikit-learn\n",
    "\n",
    "The following example is taken from the `scikit-learn` documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score       =  0.9904855148891651\n",
      "Coef        =  [0.12109212 0.15836976]\n",
      "Intercept   =  2.0885914156053205\n",
      "Predictions = [10.67658784 21.87505182]\n"
     ]
    }
   ],
   "source": [
    "# from https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PoissonRegressor.html\n",
    "\n",
    "clf = linear_model.PoissonRegressor()\n",
    "X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n",
    "y = [12, 17, 22, 21]\n",
    "clf.fit(X, y)\n",
    "print('Score       = ', clf.score(X, y))\n",
    "print('Coef        = ', clf.coef_)\n",
    "print('Intercept   = ', clf.intercept_)\n",
    "print('Predictions =', clf.predict([[1, 1], [3, 4]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML inference in Poisson regression (ctd)\n",
    "\n",
    "\n",
    "* Recall that if $E_{P_{n}}\\log p\\left(  \\beta,z\\right)  $ is the\n",
    "log-likelihood of the sample, and setting $l\\left(  \\beta,z\\right)  =\\log\n",
    "p\\left(  \\beta,z\\right)  $ we get\n",
    "\n",
    "$$\n",
    "E_{P_{n}}\\left[  \\partial_{\\beta}l\\left(  \\beta_{n},z\\right)  \\right]   \n",
    "=0\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "E_{P}\\left[  \\partial_{\\beta}l\\left(  \\beta,z\\right)  \\right]     =0\n",
    "$$\n",
    "\n",
    "thus\n",
    "$E_{P}\\left[  \\partial_{\\beta}l\\left(  \\beta_{n},z\\right)  \\right]\n",
    "-E_{P}\\left[  \\partial_{\\beta}l\\left(  \\beta,z\\right)  \\right]  =E_{P}\\left[\n",
    "\\partial_{\\beta}l\\left(  \\beta_{n},z\\right)  \\right]  -E_{P_{n}}\\left[\n",
    "\\partial_{\\beta}l\\left(  \\beta_{n},z\\right)  \\right]\n",
    "$\n",
    "therefore\n",
    "\n",
    "$$\n",
    "\\left(  \\beta_{n}-\\beta\\right)  E_{P}\\left[  \\partial_{\\beta}^{2}l\\left(\n",
    "\\beta_{n},z\\right)  \\right]  =-\\frac{1}{\\sqrt{n}}g_{n}\\left(  \\partial_{\\beta\n",
    "}l\\left(  \\beta,z\\right)  \\right)\n",
    "$$\n",
    "where $g_{n}f=\\sqrt{n}\\left(  E_{P_{n}}f-E_{P}f\\right)$.\n",
    "\n",
    "* Thus\n",
    "$$\n",
    "\\beta_{n}-\\beta=-\\frac{1}{\\sqrt{n}}\\left(  E_{P}\\left[  \\partial_{\\beta}%\n",
    "^{2}l\\left(  \\beta,z\\right)  \\right]  \\right)  ^{-1}g_{n}\\left(\n",
    "\\partial_{\\beta}l\\left(  \\beta,z\\right)  \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "* Hence\n",
    "$$\n",
    "V \\left(\\beta_{n}-\\beta\\right)   =\\frac{1}{n}\\left(  E_{P}\\left[\n",
    "\\partial_{\\beta}^{2}l\\left(  \\beta,z\\right)  \\right]  \\right)  ^{-1}  \\times E_{P}\\left(  \\partial_{\\beta}l\\left(  \\beta,z\\right)  \\left(\n",
    "\\partial_{\\beta}l\\left(  \\beta,z\\right)  \\right)  ^{\\top}\\right)  \\times\\left(  E_{P}\\left[  \\partial_{\\beta}^{2}l\\left(  \\beta,z\\right)\n",
    "\\right]  \\right)  ^{-1}\n",
    "$$\n",
    "\n",
    "\n",
    "* And because at the ML parameter\n",
    "$$\n",
    "E_{P}\\left(  \\partial_{\\beta}l\\left(  \\beta,z\\right)  \\left(  \\partial_{\\beta\n",
    "}l\\left(  \\beta,z\\right)  \\right)  ^{\\top}\\right)  =E_{P}\\left[\n",
    "\\partial_{\\beta}^{2}l\\left(  \\beta,z\\right)  \\right]  ,\n",
    "$$\n",
    "we have thus\n",
    "$$\n",
    "V\\left(  \\beta_{n}-\\beta\\right)  =\\frac{1}{n}\\left(  E_{P}\\left[\n",
    "\\partial_{\\beta}^{2}l\\left(  \\beta,z\\right)  \\right]  \\right)  ^{-1}.\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of GLM\n",
    "\n",
    "\n",
    "\n",
    "* Actually, we don't need to assume that $y\\sim Poisson\\left(\n",
    "\\exp(x^{\\top}\\beta)\\right)  $ to estimate $\\beta$.\n",
    "\n",
    "* Consider $X$ the matrix obtained by stacking the rows $x_{i}^{\\top}$ on\n",
    "top of each other. Compute\n",
    "\n",
    "$$\n",
    "\\max_{\\beta}\\left\\{  y^{\\top}X\\beta-1^{\\top}\\exp\\left(  X\\beta\\right)\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "and define $\\overline{y}=\\exp\\left(  X\\beta\\right)  $ the predictor of $y$.\n",
    "One has\n",
    "\n",
    "$$\n",
    "\\sum_{i}y_{i}X_{ik}=\\sum_{i}\\overline{y}_{i}X_{ik}~\\forall k\n",
    "$$\n",
    "and therefore $\\beta$ is obtained by matching the predicted moments with the\n",
    "observed ones\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[  y_{i}x_{i}\\right]  =\\mathbb{E}\\left[  \\overline{y}_{i}%\n",
    "x_{i}\\right]  .\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference in GLM\n",
    "\n",
    "\n",
    "* While the point estimate is unchanged wrt the Poisson regression, the\n",
    "inference is changed as soon as one departs from the assumption that\n",
    "$Var\\left(  y|x\\right)  =x^{\\top}\\beta$. Assume $Var\\left(  y|x\\right)\n",
    "=V\\left(  y|x\\right)  $.\n",
    "\n",
    "* The estimation of $\\beta$ is now seen as what is called an\n",
    "*M-estimation* procedure\n",
    "$$\n",
    "\\max_{\\beta}\\frac{1}{n}\\sum_{i=1}^{n}F\\left(  z_{i},\\theta\\right)  .\n",
    "$$\n",
    "\n",
    "\n",
    "* The derivation done for MLE applies replacing $\\partial_{\\beta}l\\left(\n",
    "\\beta,z_{i}\\right)  =\\partial_{\\beta}\\log p\\left(  \\beta,z_{i}\\right)  $ by\n",
    "$\\partial_{\\beta}l\\left(  \\beta,z_{i}\\right)  =\\left(  y_{i}-\\exp\\left(\n",
    "x_{i}^{\\top}\\beta\\right)  \\right)  x_{i}$ with the provision that\n",
    "$E_{P}\\left[  \\partial_{\\beta}^{2}l\\left(  \\beta,z\\right)  \\right]  \\neq\n",
    "E_{P}\\left[  \\partial l\\left(  \\beta,z\\right)  \\partial l\\left(\n",
    "\\beta,z\\right)  ^{\\top}\\right]  $. Hence\n",
    "\n",
    "$$\n",
    "V\\left(  \\beta_{n}-\\beta\\right)   =\\frac{1}{n}\\left(  E_{P}\\left[\n",
    "\\partial_{\\beta}^{2}l\\left(  \\beta,z\\right)  \\right]  \\right)  ^{-1} \\times E_{P}\\left(  \\partial_{\\beta}l\\left(  \\beta,z\\right)  \\left(\n",
    "\\partial_{\\beta}l\\left(  \\beta,z\\right)  \\right)  ^{\\top}\\right) \\times\\left(  E_{P}\\left[  \\partial_{\\beta}^{2}l\\left(  \\beta,z\\right)\n",
    "\\right]  \\right)  ^{-1}%\n",
    "$$\n",
    "\n",
    "\n",
    "* We have\n",
    "$$\n",
    "E_{P}\\left[  \\partial_{\\beta}^{2}l\\left(  \\beta,z\\right)  \\right]  =E\\left[\n",
    "\\exp\\left(  x^{\\top}\\beta\\right)  xx^{\\top}\\right]\n",
    "$$\n",
    "and\n",
    "\n",
    "$$\n",
    "E_{P}\\left[  \\partial_{\\beta}l\\left(  \\beta,z\\right)  \\left(  \\partial_{\\beta\n",
    "}l\\left(  \\beta,z\\right)  \\right)  ^{\\top}\\right]  =E\\left[  \\left(\n",
    "y-\\exp\\left(  x^{\\top}\\beta\\right)  \\right)  ^{2}xx^{\\top}\\right] \\\\  =E\\left[  V\\left(  y|x\\right)  xx^{\\top}\\right]  .\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson regression and duality\n",
    "\n",
    "\n",
    "Consider $y\\in\\mathbb{R}_{+}^{n}$, $\\beta\\in R^{k}$ and $X$ a $n\\times k$ matrix\n",
    "\n",
    "\n",
    "> <span style=\"color:yellow\"> **Theorem (Poisson duality)**. The primal problem\n",
    "$$\n",
    "\\max_{\\beta}\\left\\{  y^{\\top}X\\beta-1^{\\top}\\exp\\left(  X\\beta\\right)\n",
    "\\right\\}\n",
    "$$\n",
    "has dual\n",
    "$$\n",
    "\\min_{\\bar{y}\\in\\mathbb{R}_{+}^{n}}  \\bar{y}^{\\top}\\left(  \\ln\\bar\n",
    "{y}-1\\right) \\\\\n",
    "s.t.   X^{\\top}\\left(  z-\\bar{y}\\right)  =0.\n",
    "$$</span>\n",
    "\n",
    "\n",
    "**Proof**. Start from the latter expression and write the Lagrangian for\n",
    "the problem \n",
    "\n",
    "$$\n",
    "\\min_{\\bar{y}\\geq0}\\max_{\\beta}\\bar{y}^{\\top}\\left(  \\ln\\bar{y}-1\\right)\n",
    "-\\left(  \\bar{y}-y\\right)  ^{\\top}X\\beta =\\max_{\\beta}y^{\\top}X\\beta+\\min_{\\bar{y}\\geq0}\\left\\{  \\bar{y}^{\\top\n",
    "}\\left(  \\ln\\bar{y}-1\\right)  -\\bar{y}^{\\top}X\\beta\\right\\}\n",
    "$$\n",
    "\n",
    "has $\\ln\\bar{y}=X\\beta$ and $\\bar{y}^{\\top}\\left(  \\ln\\bar{y}-1\\right)\n",
    "-\\bar{y}^{\\top}X\\beta=-\\bar{y}^{\\top}1=-1^{\\top}\\exp\\left(  X\\beta\\right)  $\n",
    "and hence this is\n",
    "\n",
    "$$\n",
    "\\max_{\\beta}y^{\\top}X\\beta-1^{\\top}\\exp\\left(  X\\beta\\right)  .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete choice models\n",
    "\n",
    "## Multinomial logit model and logistic regression\n",
    "\n",
    "\n",
    "* Consider the logit model\n",
    "$$\n",
    "\\sum_{k}\\Phi_{ij}^{k}\\lambda_{k}+\\varepsilon_{ij}$$\n",
    "where $\\varepsilon_{ij}$ are iid Gumbel distributions, i.e. of c.d.f.\n",
    "$\\exp\\left(  -\\exp\\left(  -x\\right)  \\right)  $.\n",
    "\n",
    "* The conditional probability that $i$ chooses $j$ is\n",
    "$$\n",
    "\\pi_{ij}=\\frac{\\exp\\left(  \\sum_{k}\\Phi_{ij}^{k}\\lambda_{k}\\right)  }{\\sum\n",
    "_{j}\\exp\\left(  \\sum_{k}\\Phi_{ij}^{k}\\lambda_{k}\\right)  }\n",
    "    $$\n",
    "and therefore the conditional likelihood associated with $j$ is the logistic\n",
    "regression\n",
    "$$\n",
    "l_{ij}\\left(  \\lambda\\right)  =\\log\\pi_{ij}=\\sum_{k}\\Phi_{ij}^{k}\\lambda\n",
    "_{k}-\\log\\sum_{j}\\exp\\left(  \\sum_{k}\\Phi_{ij}^{k}\\lambda_{k}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "* As a result, if $J\\left(  i\\right)  $ is the actual choice of $i$, and\n",
    "$\\hat{\\pi}_{ij}=1\\left\\{  j=J\\left(  i\\right)  \\right\\}  $, the logistic\n",
    "regression can be expressed as\n",
    "\n",
    "$$\n",
    "l\\left(  \\lambda\\right)  =\\hat{\\pi}^{\\top}\\Phi\\lambda-\\sum_{i}\\log\\sum_{j}%\n",
    "\\exp\\left(  \\left(  \\Phi\\lambda\\right)  _{ij}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "* This is *almost*, but *not quite* the form of a GLM $-$ notice\n",
    "the $\\log$. To make the precise connection with GLM/Poisson regression, we\n",
    "need to introduce *individual fixed effects*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regresssion as a GLM\n",
    "\n",
    "\n",
    "* Introduce a fixed effect $u_{i}$ and let $\\beta=\\left(  \\lambda^{\\top\n",
    "},u^{\\top}\\right)  ^{\\top}$. We rewrite $\\left(  \\lambda,u\\right)\n",
    "\\rightarrow\\left(  \\left(  \\Phi\\lambda\\right)  _{ij}-u_{i}\\right)  _{ij}$ in a\n",
    "matrix form by defining\n",
    "\n",
    "$$\n",
    "X=%\n",
    "\\begin{pmatrix}\n",
    "\\Phi, -I_{n}\\otimes 1_{n}%\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "where $\\otimes$ is the Kronecker product and we have\n",
    "$$\n",
    "X\\beta=vec\\left(  \\left(  \\left(  \\Phi\\lambda\\right)  _{ij}-u_{i}\\right)\n",
    "_{ij}\\right)  .\n",
    "$$\n",
    "\n",
    "\n",
    "* The Poisson regression of $\\hat{\\pi}_{ij}$ on $X$ yields\n",
    "$$\n",
    "\\max_{\\lambda,u}\\left\\{  -\\sum_{ij}\\exp\\left(  \\left(  \\Phi\\lambda\\right)\n",
    "_{ij}-u_{i}\\right)  +\\sum_{ij}\\hat{\\pi}_{ij}\\left(  \\left(  \\Phi\n",
    "\\lambda\\right)  _{ij}-u_{i}\\right)  \\right\\}\n",
    "$$\n",
    "therefore\n",
    "$$\n",
    "\\max_{\\lambda,u}\\left\\{  -\\sum_{ij}\\exp\\left(  \\left(  \\Phi\\lambda\\right)\n",
    "_{ij}-u_{i}\\right)  +\\sum_{ij}\\hat{\\pi}_{ij}\\left(  \\Phi\\lambda\\right)\n",
    "_{ij}-\\sum_{i}u_{i}\\right\\}  .\n",
    "$$\n",
    "\n",
    "* Taking first order conditions in $u_{i}$ we get\n",
    "$$\n",
    "\\sum_{j}\\exp\\left(  \\left(  \\Phi\\lambda\\right)  _{ij}-u_{i}\\right)  =1\n",
    "$$\n",
    "\n",
    "\n",
    "* Therefore, $u_{i}=\\log\\sum_{j}\\exp\\left(  \\left(  \\Phi\\lambda\\right)\n",
    "_{ij}\\right)  $ and the problem becomes the MLE in the multinomial logit\n",
    "model\n",
    "$$\n",
    "\\max_{\\lambda,u}\\left\\{  \\sum_{ij}\\hat{\\pi}_{ij}\\left(  \\Phi\\lambda\\right)\n",
    "_{ij}-\\sum_{i}\\log\\sum_{j}\\exp\\left(  \\left(  \\Phi\\lambda\\right)\n",
    "_{ij}\\right)  \\right\\}  .\n",
    "$$\n",
    "\n",
    "\n",
    "* To summarize: \n",
    "> <span style=\"color:yellow\">**Logistic regression = GLM + fixed effect**.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete choice application \n",
    "\n",
    "See Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trade models\n",
    "\n",
    "## Gravity equation\n",
    "\n",
    "\n",
    "* The gravity models seeks to explain the trade flows $\\hat{\\pi}_{ij}$\n",
    "from country $i$ to country $j$ by using various measures of proximity between\n",
    "these countries. (We assume $\\hat{\\pi}_{ii}=0$.)\n",
    "\n",
    "* We denote\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array} \\\\\n",
    "p_{i}=\\sum_{j}\\hat{\\pi}_{ij}\\\\\n",
    "q_{j}=\\sum_{i}\\hat{\\pi}_{ij}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "the total volume of the exports of country $i$ and of the imports of country\n",
    "$j$, respectively.\n",
    "\n",
    "* We have the accounting equation\n",
    "$$\n",
    "\\sum_{i}p_{i}=\\sum_{ij}\\hat{\\pi}_{ij}=\\sum_{j}q_{j}%\n",
    "$$\n",
    "and (by simply rescaling) we can without loss of generality assume that these\n",
    "quantities sum to one.\n",
    "\n",
    "* The *gravity model* assumes\n",
    "$$\n",
    "E\\left[  \\hat{\\pi}_{ij}|\\Phi\\right]  =\\exp\\left(  \\left(  \\Phi\\lambda\\right)\n",
    "_{ij}-u_{i}-v_{j}\\right)\n",
    "$$\n",
    "\n",
    "where $u_{i}$ and $v_{j}$ are resistance terms, or country-specific fixed\n",
    "effects. This is a GLM with two-way fixed effects. Need to rewrite $\\left(\n",
    "\\lambda,u,v\\right)  \\rightarrow\\left(  \\left(  \\Phi\\lambda\\right)  _{ij}%\n",
    "-u_{i}-v_{j}\\right)  _{ij}$ in a matrix form, again using vectorization and\n",
    "Kronecker products.\n",
    "\n",
    "* Hence:\n",
    "> <span style=\"color:yellow\">**Gravity equation = GLM + 2-ways fixed effect** </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed effects and Kronecker products\n",
    "\n",
    "\n",
    "* Set up\n",
    "$$\n",
    "X=%\n",
    "\\begin{pmatrix}\n",
    "\\Phi & -1_{n}\\otimes I_{n} & -I_{n}\\otimes1_{n}%\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "* Taking parameter $\\beta=\\left(  \\lambda^{\\top},u^{\\top},v^{\\top}\\right)\n",
    "^{\\top}$, we have\n",
    "$$\n",
    "X\\beta=vec\\left(  \\left(  \\left(  \\Phi\\lambda\\right)  _{ij}-u_{i}%\n",
    "-v_{j}\\right)  _{ij}\\right)  .\n",
    "$$\n",
    "\n",
    "\n",
    "* Therefore rewrite our regression with $y_{ij}=\\hat{\\pi}_{ij}$, and\n",
    "consider the Poisson regression\n",
    "\n",
    "$$\n",
    "\\max_{\\beta}\\left\\{  y^{\\top}X\\beta-1^{\\top}\\exp\\left(  X\\beta\\right)\n",
    "\\right\\}\n",
    "$$\n",
    "which becomes\n",
    "$$\n",
    "\\max_{\\lambda,u,v}\\left\\{  \\sum_{ij}\\hat{\\pi}_{ij}\\left(  \\left(  \\Phi\n",
    "\\lambda\\right)  _{ij}-u_{i}-v_{j}\\right)  -\\sum_{ij}\\exp\\left(  \\left(\n",
    "\\Phi\\lambda\\right)  _{ij}-u_{i}-v_{j}\\right)  \\right\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gravity as max-entropy\n",
    "\n",
    "\n",
    "* By the GLM duality theorem, the dual to this program is\n",
    "$$ \\min_{\\pi_{ij}\\geq0}\\sum_{ij}\\pi_{ij}\\ln\\pi_{ij}-\\sum_{ij}\\pi_{ij}\\\\\n",
    "s.t.~ \\sum_{j}\\pi_{ij}=p_{i},~\\sum_{i}\\pi_{ij}=q_{j}\\\\\n",
    " \\sum_{ij}\\pi_{ij}\\Phi_{ij}^{k}=\\sum_{ij}\\hat{\\pi}_{ij}\\Phi_{ij}^{k}\n",
    "$$\n",
    "\n",
    "* But as $\\sum_{ij}\\pi_{ij}=1$, we interpret the previous program as\n",
    "looking among the $\\pi_{ij}$ that has the same margins and moments as\n",
    "$\\hat{\\pi}$, the one that maximizes entropy $-\\sum_{ij}\\pi_{ij}\\ln\\pi_{ij}$.\n",
    "Rewrite as\n",
    "\n",
    "$$\n",
    "\\max_{\\pi_{ij}\\geq0}\\left\\{  -\\sum_{ij}\\pi_{ij}\\ln\\pi_{ij}\\right\\} \\\\\n",
    "s.t.~  \\sum_{j}\\pi_{ij}=p_{i},~\\sum_{i}\\pi_{ij}=q_{j}\\\\\n",
    "\\sum_{ij}\\pi_{ij}\\Phi_{ij}^{k}=\\sum_{ij}\\hat{\\pi}_{ij}\\Phi_{ij}^{k}%\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trade application\n",
    "\n",
    "See Jupyter notebook. XXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching models\n",
    "\n",
    "\n",
    "* Becker (1973) describes the following model of the labor market, the\n",
    "marriage market, and other matching markets. Consider a population with a\n",
    "share $p_{i}$ men of type $i$ and a share $q_{j}$ of women of type $j$,\n",
    "assuming that men and women come in equal numbers. Assume that if $i$ and $j$\n",
    "match, this generates a joint surplus (sum of their utilities) $\\Phi_{ij}$.\n",
    "\n",
    "\n",
    "* Let $\\pi_{ij}$ be the fraction of couples $ij$ that are formed at\n",
    "equilibrium. Becker shows that the equilibrium maximizes the total surplus\n",
    "$\\sum_{ij}\\pi_{ij}\\Phi_{ij}$ out of all the feasible matchings, which are\n",
    "those with\n",
    "$$\n",
    "\\sum_{j}\\pi_{ij}=p_{i}\\text{ and }\\sum_{i}\\pi_{ij}=q_{j}.\n",
    "$$\n",
    "\n",
    "* Therefore, the equilibrium matching $\\pi_{ij}$ should solve\n",
    "$$\n",
    "\\max_{\\pi_{ij}\\geq0} \\sum_{ij}\\pi_{ij}\\Phi_{ij}\\\\\n",
    "s.t.~  \\sum_{j}\\pi_{ij}=p_{i}\\text{ and }\\sum_{i}\\pi_{ij}=q_{j}.\n",
    "$$\n",
    "\n",
    "* Choo and Siow (2006) and Dupuy and Galichon (2015) consider a variant\n",
    "of this model with entropic regularization\n",
    "\n",
    "$$\n",
    "\\max_{\\pi_{ij}\\geq0} \\sum_{ij}\\pi_{ij}\\Phi_{ij}-\\sigma\\sum_{ij}\\pi_{ij}%\n",
    "\\ln\\pi_{ij}\\\\\n",
    "s.t.~  \\sum_{j}\\pi_{ij}=p_{i}\\text{ and }\\sum_{i}\\pi_{ij}=q_{j}.\n",
    "$$\n",
    "\n",
    "\n",
    "* We shall see that we can parametrically estimate $\\Phi$ in this model by\n",
    "the same tools as for the gravity equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to gravity equation\n",
    "\n",
    "\n",
    "* Consider the previous program\n",
    "$$ \\max_{\\pi_{ij}\\geq0}\\left\\{  -\\sum_{ij}\\pi_{ij}\\ln\\pi_{ij}\\right\\} \\\\\n",
    "s.t.~ \\sum_{j}\\pi_{ij}=p_{i},~\\sum_{i}\\pi_{ij}=q_{j}\\\\\n",
    " \\sum_{ij}\\pi_{ij}\\Phi_{ij}^{k}=\\sum_{ij}\\hat{\\pi}_{ij}\\Phi_{ij}^{k}%\n",
    "$$\n",
    "\n",
    "and rewrite as\n",
    "$$\n",
    "\\max_{\\pi_{ij}\\geq0}\\left\\{  -\\sum_{ij}\\pi_{ij}\\ln\\pi_{ij}+\\min_{\\left(\n",
    "\\lambda_{k}\\right)  }\\left\\{  \\sum_{ijk}\\left(  \\pi_{ij}-\\hat{\\pi}%\n",
    "_{ij}\\right)  \\Phi_{ij}^{k}\\lambda_{k}\\right\\}  \\right\\} \\\\\n",
    "s.t.~ \\sum_{j}\\pi_{ij}=p_{i},~\\sum_{i}\\pi_{ij}=q_{j}%\n",
    "$$\n",
    "\n",
    "* By the strong duality theorem, this is\n",
    "$$\n",
    "\\min_{\\left(  \\lambda_{k}\\right)  }\\left\\{  W\\left(  \\beta\\right)  -\\sum\n",
    "_{ijk}\\hat{\\pi}_{ij}\\Phi_{ij}^{k}\\lambda_{k}\\right\\}\n",
    "$$\n",
    "where we recover\n",
    "$$\n",
    "W\\left(  \\beta\\right)  =\\max_{\\pi_{ij}\\geq0} \\left\\{  \\sum_{ijk}\\pi\n",
    "_{ij}\\Phi_{ij}^{k}\\lambda_{k}-\\sum_{ij}\\pi_{ij}\\ln\\pi_{ij}\\right\\} \\\\\n",
    "s.t.~ \\sum_{j}\\pi_{ij}=p_{i},~\\sum_{i}\\pi_{ij}=q_{j}%\n",
    "$$\n",
    "which is the matching surplus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching application\n",
    "\n",
    "* See Jupyter notebook. XXXXXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic models\n",
    "\n",
    "\n",
    "## Rust's model of dynamic discrete choice\n",
    "\n",
    "\n",
    "* In a dynamic discrete choice model (Rust 1987), the decision-maker in a\n",
    "state $x$ chooses $j$ based on the short-term payoff $\\phi_{xj}+\\varepsilon\n",
    "_{xj}$, where $\\varepsilon_{xj}$ is Gumbel, but also on the expected value of\n",
    "being in a different state $x^{\\prime}$ at the next period.\n",
    "\n",
    "* The probability of being in state $x^{\\prime}$ conditional on being in\n",
    "state $x$ and having chosen $j$ is $P_{x^{\\prime}|xj}$.\n",
    "\n",
    "\n",
    "* In a stationary equilibrium, one has\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array} \\\\\n",
    "\\pi_{j|x}=\\exp\\left(  \\left(  \\Phi\\lambda\\right)  _{xj}+\\beta\\sum_{x^{\\prime}%\n",
    "}P_{x^{\\prime}|xj}u_{x^{\\prime}}-u_{x}\\right) \\\\\n",
    "u_{x}=\\log\\sum_{j}\\exp\\left(  \\left(  \\Phi\\lambda\\right)  _{xj}+\\beta\n",
    "\\sum_{x^{\\prime}}P_{x^{\\prime}|xj}u_{x^{\\prime}}\\right)\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "\n",
    "* The MLE is therefore\n",
    "    \n",
    "$$\n",
    "\\max_{\\beta,u}  \\sum_{i}\\left(  \\Phi\\lambda\\right)  _{x_{i}J_{i}}+\\beta\n",
    "\\sum_{x^{\\prime}}P_{x^{\\prime}|x_{i}J_{i}}u_{x^{\\prime}}-u_{x_{i}}\\\\\n",
    "s.t.~   u_{x}=\\log\\sum_{j}\\exp\\left(  \\left(  \\Phi\\lambda\\right)\n",
    "_{xj}+\\beta\\sum_{x^{\\prime}}P_{x^{\\prime}|xj}u_{x^{\\prime}}\\right)\n",
    "$$\n",
    "\n",
    "this is Rust's \"nested fixed point\" algorithm: the \"inner loop\" looks for a\n",
    "fixed point $u_{x}$, while the \"outer loop\" seeks to maximize the likelihood.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
